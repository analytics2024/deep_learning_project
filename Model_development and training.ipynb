{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df8e9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618b8490e6db4834aab19cf372903f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Enter the model for feature extraction: ', style=TextStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a0ded6d1d34d5b9e5bba474ee22402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Enter the Sequential model : ', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/b0mdn8zd7msdn25jg6z_ln3c0000gn/T/ipykernel_11274/266145124.py:26: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  text_input1.on_submit(handle_submit)\n",
      "/var/folders/9d/b0mdn8zd7msdn25jg6z_ln3c0000gn/T/ipykernel_11274/266145124.py:27: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  text_input2.on_submit(handle_submit)\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "text_input1 = widgets.Text()\n",
    "text_input1.style.description_width = 'initial'  # Set description width to initial, allowing longer descriptions\n",
    "text_input1.description = 'Enter the model for feature extraction: '\n",
    "\n",
    "text_input2 = widgets.Text()\n",
    "text_input2.style.description_width = 'initial'  # Set description width to initial, allowing longer descriptions\n",
    "text_input2.description = 'Enter the Sequential model : '\n",
    "\n",
    "# Define variables to store the input values\n",
    "feature_model = None\n",
    "sequence_model = None\n",
    "\n",
    "# Display the widgets\n",
    "display(text_input1)\n",
    "display(text_input2)\n",
    "\n",
    "# Define a function to handle the input\n",
    "def handle_submit(sender):\n",
    "    global feature_model, sequence_model\n",
    "    feature_model = text_input1.value\n",
    "    sequence_model = text_input2.value\n",
    "\n",
    "# Set the function to handle input submission\n",
    "text_input1.on_submit(handle_submit)\n",
    "text_input2.on_submit(handle_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1eb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load as pickle_load\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def captions_to_list(caption_dictionary):\n",
    "    # Return: a list of all captions from the caption_dict\n",
    "    captions = []\n",
    "    for caption_list in caption_dictionary.values():\n",
    "        for c in caption_list:\n",
    "            captions.append(c)\n",
    "    return captions\n",
    "\n",
    "\n",
    "def create_tokenizer(caption_dictionary, num_vocab=None):\n",
    "    \"\"\"\n",
    "    Input: caption dictionary, num_vocab\n",
    "    Output: Tokenizer fitted on the captions in the dictionary, with maximum number of vocab = num_vocab\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=num_vocab, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "    captions = captions_to_list(caption_dictionary)\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Input: filename of dataset\n",
    "    Output: A list of identifiers in the dataset\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        dataset.append(line.split('.')[0])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_image_features(dataset, file_name):\n",
    "    \"\"\"\n",
    "    Input: dataset (list of identifier)\n",
    "    Output: The VGG-16 features according to the identifiers\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        features = pickle_load(file)\n",
    "    features = {photo_id: features[photo_id] for photo_id in dataset}\n",
    "\n",
    "    return features\n",
    "\n",
    "def load_captions(dataset, wrapping=0):\n",
    "    \"\"\"\n",
    "    Input: dataset (list of identifier), wrapping (by startseq / endseq)\n",
    "    Output: The caption dictionary according to the identifiers, with / without wrapping of (startseq, endseq)\n",
    "    \"\"\"\n",
    "    with open(\"captions.pkl\", \"rb\") as file:\n",
    "        caption_dictionary = pickle_load(file)\n",
    "    caption_dictionary = {photo_id: caption_dictionary[photo_id] for photo_id in dataset}\n",
    "\n",
    "    if wrapping:\n",
    "        for photo_id, caption_list in caption_dictionary.items():\n",
    "            for i in range(len(caption_list)):\n",
    "                tmp = caption_list[i].split()\n",
    "                caption_list[i] = '<startseq> ' + ' '.join(tmp) + ' <endseq>'\n",
    "\n",
    "    return caption_dictionary\n",
    "\n",
    "\n",
    "\n",
    "def define_model_lstm(max_length, vocab_size, dp_rate=0.1, embed_size=100, embedding_matrix=None):\n",
    "    img_inputs = keras.layers.Input(shape=(4096,))\n",
    "    img_dp1 = keras.layers.Dropout(rate=dp_rate)(img_inputs)\n",
    "    img_dense = keras.layers.Dense(64)(img_dp1)\n",
    "    img_bn1 = keras.layers.BatchNormalization()(img_dense)\n",
    "    img_outputs = keras.layers.Activation(activation='relu')(img_bn1)\n",
    "    #RepeatVector for matching timestep\n",
    "    img_rep = keras.layers.RepeatVector(n=max_length)(img_outputs)\n",
    "    \n",
    "    text_inputs = keras.layers.Input(shape=(max_length,))\n",
    "    if embedding_matrix is None:\n",
    "        text_embed = keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                            output_dim=embed_size, \n",
    "                                            mask_zero=True,\n",
    "                                            name='text_embedding')(text_inputs)\n",
    "        \n",
    "    else:\n",
    "        text_embed = keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                            output_dim=embed_size, \n",
    "                                            mask_zero=True,\n",
    "                                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                            trainable=False,\n",
    "                                            name='text_embedding')(text_inputs)\n",
    "            \n",
    "    text_dp1 = keras.layers.Dropout(rate=dp_rate)(text_embed)\n",
    "    text_lstm = keras.layers.LSTM(64, return_sequences=True)(text_dp1)\n",
    "    \n",
    "    decoder_inputs = keras.layers.Concatenate()([img_rep, text_lstm])\n",
    "    decoder_dp1 = keras.layers.Dropout(rate=dp_rate, noise_shape=(None, 1, 128))(decoder_inputs)\n",
    "    decoder_dense1 = keras.layers.Dense(256)(decoder_dp1)\n",
    "    decoder_dp2 = keras.layers.Dropout(rate=dp_rate, noise_shape=(None, 1, 256))(decoder_dense1)\n",
    "    decoder_relu1 = keras.layers.Activation(activation='relu')(decoder_dp2)\n",
    "    decoder_outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder_relu1)\n",
    "    \n",
    "    model = keras.models.Model(inputs = [img_inputs, text_inputs], outputs=decoder_outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def define_model_rnn(max_length, vocab_size, dp_rate=0.1, embed_size=100, embedding_matrix = None):\n",
    "    img_inputs = keras.layers.Input(shape=(4096,))\n",
    "    img_dp1 = keras.layers.Dropout(rate=dp_rate)(img_inputs)\n",
    "    img_dense = keras.layers.Dense(64)(img_dp1)\n",
    "    img_bn1 = keras.layers.BatchNormalization()(img_dense)\n",
    "    img_outputs = keras.layers.Activation(activation='relu')(img_bn1)\n",
    "    img_rep = keras.layers.RepeatVector(n=max_length)(img_outputs)\n",
    "    \n",
    "    text_inputs = keras.layers.Input(shape=(max_length,))\n",
    "    if embedding_matrix is None:\n",
    "        print('embedding_matrix is not available')\n",
    "        text_embed = keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                            output_dim=embed_size, \n",
    "                                            mask_zero=True,\n",
    "                                            name='text_embedding')(text_inputs)\n",
    "        \n",
    "    else:\n",
    "        print('embedding_matrix is available')\n",
    "        text_embed = keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                            output_dim=embed_size, \n",
    "                                            mask_zero=True,\n",
    "                                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                            trainable=False,\n",
    "                                            name='text_embedding')(text_inputs)\n",
    "            \n",
    "    text_dp1 = keras.layers.Dropout(rate=dp_rate)(text_embed)\n",
    "    text_rnn = keras.layers.SimpleRNN(64, return_sequences=True)(text_dp1)\n",
    "    \n",
    "    decoder_inputs = keras.layers.Concatenate()([img_rep, text_rnn])\n",
    "    decoder_dp1 = keras.layers.Dropout(rate=dp_rate, noise_shape=(None, 1, 128))(decoder_inputs)\n",
    "    decoder_dense1 = keras.layers.Dense(256)(decoder_dp1)\n",
    "    decoder_dp2 = keras.layers.Dropout(rate=dp_rate, noise_shape=(None, 1, 256))(decoder_dense1)\n",
    "    decoder_relu1 = keras.layers.Activation(activation='relu')(decoder_dp2)\n",
    "    decoder_outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder_relu1)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[img_inputs, text_inputs], outputs=decoder_outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_dataset(caption_dictionary, features, tokenizer, max_length, vocab_size, num_photos=32):\n",
    "    \"\"\"\n",
    "    A generator of dataset for model training / validation.\n",
    "    Input: train / val (caption_dict & features)\n",
    "    Yield: a batch of [[X_img, X_text], Y] as the model input for model.fit_generator() / model.evaluate_generator()\n",
    "    \"\"\"\n",
    "\n",
    "    photo_ids = list(caption_dictionary.keys())\n",
    "    while True:\n",
    "        s = np.random.choice(np.arange(len(photo_ids)), size=len(photo_ids), replace=False)\n",
    "\n",
    "        count = 0\n",
    "        X_img = []\n",
    "        X_text = []\n",
    "        Y = []\n",
    "        while True:\n",
    "            s1 = s[count]\n",
    "            photo_id = photo_ids[s1]\n",
    "            caption_list = caption_dictionary[photo_id]\n",
    "\n",
    "            s2 = np.random.choice(np.arange(len(caption_list)), size=1, replace=True)[0]\n",
    "            caption = caption_list[s2]\n",
    "            encoded = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "            tmp_text, tmp_Y = encoded[:-1], encoded[1:]\n",
    "            padded_text = keras.preprocessing.sequence.pad_sequences([tmp_text], maxlen=max_length, padding='pre')[0]\n",
    "            padded_Y = keras.preprocessing.sequence.pad_sequences([tmp_Y], maxlen=max_length, padding='pre')[0]\n",
    "            padded_Y = tf.keras.utils.to_categorical(padded_Y, num_classes=vocab_size)\n",
    "\n",
    "            X_img.append(features[photo_id][0])\n",
    "            X_text.append(padded_text)\n",
    "            Y.append(padded_Y)\n",
    "\n",
    "            count += 1\n",
    "            if count % num_photos == 0:\n",
    "                yield [[np.array(X_img), np.array(X_text)], np.array(Y)]\n",
    "                X_img = []\n",
    "                X_text = []\n",
    "                Y = []\n",
    "            if count >= len(photo_ids):\n",
    "                if len(Y) > 0:\n",
    "                    yield [[np.array(X_img), np.array(X_text)], np.array(Y)]\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_caption(model, tokenizer, max_length, vocab_size, feature):\n",
    "    \"\"\"\n",
    "    Input: model, photo feature: shape=[1,4096]\n",
    "    Return: A generated caption of that photo feature. Remove the startseq and endseq token.\n",
    "    \"\"\"\n",
    "\n",
    "    caption = \"<startseq>\"\n",
    "    while True:\n",
    "        encoded = tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded = keras.preprocessing.sequence.pad_sequences([encoded], maxlen=max_length, padding='pre')[0]\n",
    "        padded = padded.reshape((1, max_length))\n",
    "\n",
    "        pred_Y = model.predict([feature, padded])[0, -1, :]\n",
    "        next_word = tokenizer.index_word[pred_Y.argmax()]\n",
    "\n",
    "        caption = caption + ' ' + next_word\n",
    "\n",
    "        if next_word == '<endseq>' or len(caption.split()) >= max_length:\n",
    "            break\n",
    "\n",
    "    caption = caption.replace('<startseq> ', '')\n",
    "    caption = caption.replace(' <endseq>', '')\n",
    "\n",
    "    return caption\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_caption_dict, test_features, max_length, vocab_size):\n",
    "    hypo_captions_list = []\n",
    "    ref_captions_list = []\n",
    "    disp_captions_dict = {}\n",
    "    for photo_id, caption_list in test_caption_dict.items():\n",
    "        ref_captions = []\n",
    "        for c in caption_list:\n",
    "            ref_captions.append(c.split())\n",
    "        ref_captions_list.append(ref_captions)\n",
    "\n",
    "        feature = test_features[photo_id]\n",
    "        samp_caption = sample_caption(model, tokenizer, max_length, vocab_size, feature)\n",
    "        hypo_captions_list.append(samp_caption.split())\n",
    "\n",
    "        disp_captions_dict[photo_id] = samp_caption\n",
    "\n",
    "    bleu1 = np.round(corpus_bleu(ref_captions_list, hypo_captions_list, weights=(1, 0, 0, 0)), 2)\n",
    "    bleu2 = np.round(corpus_bleu(ref_captions_list, hypo_captions_list, weights=(0.5, 0.5, 0, 0)), 2)\n",
    "    bleu3 = np.round(corpus_bleu(ref_captions_list, hypo_captions_list, weights=(0.3, 0.3, 0.3, 0)), 2)\n",
    "    bleu4 = np.round(corpus_bleu(ref_captions_list, hypo_captions_list, weights=(0.25, 0.25, 0.25, 0.25)), 2)\n",
    "\n",
    "    print(\"BLEU Score on Test Set: {b1}, {b2}, {b3}, {b4}\".format(b1=bleu1, b2=bleu2, b3=bleu3, b4=bleu4))\n",
    "\n",
    "    photo_ids = list(test_caption_dict.keys())\n",
    "    np.random.seed(1)\n",
    "    samples = np.random.choice(np.arange(len(photo_ids)), 5, replace=False)\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        photo_id = photo_ids[samples[i]]\n",
    "\n",
    "        file_name = \"Flicker8k_Dataset/\" + photo_id + '.jpg'\n",
    "        img = keras.preprocessing.image.load_img(file_name)\n",
    "        plt.figure(i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.figtext(0.5, 0.01, disp_captions_dict[photo_id], wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276c2578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 4096)]               0         []                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4096)                 0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   262208    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 37)]                 0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64)                   256       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " text_embedding (Embedding)  (None, 37, 100)              300100    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 64)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 37, 100)              0         ['text_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 37, 64)               0         ['activation[0][0]']          \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 37, 64)               42240     ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 37, 128)              0         ['repeat_vector[0][0]',       \n",
      "                                                                     'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 37, 128)              0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 37, 256)              33024     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 37, 256)              0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 37, 256)              0         ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 37, 3001)             771257    ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1409085 (5.38 MB)\n",
      "Trainable params: 1108857 (4.23 MB)\n",
      "Non-trainable params: 300228 (1.15 MB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "187/187 [==============================] - 8s 34ms/step - loss: 5.1240 - accuracy: 0.1784\n",
      "Epoch 2/5\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 4.0579 - accuracy: 0.2700\n",
      "Epoch 3/5\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 3.7730 - accuracy: 0.2951\n",
      "Epoch 4/5\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 3.5870 - accuracy: 0.3143\n",
      "Epoch 5/5\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 3.4541 - accuracy: 0.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/b0mdn8zd7msdn25jg6z_ln3c0000gn/T/ipykernel_11274/2842947738.py:73: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  dev_loss = model.evaluate_generator(dev_generator, steps = val_steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 16ms/step - loss: 3.4169 - accuracy: 0.3369\n",
      "The dev_loss at 0-th epoch: [3.42 0.34]\n"
     ]
    }
   ],
   "source": [
    "#preparing the dataset\n",
    "train = load_dataset(\"Flickr8k_text/Flickr_8k.trainImages.txt\")\n",
    "dev = load_dataset(\"Flickr8k_text/Flickr_8k.devImages.txt\")\n",
    "test = load_dataset(\"Flickr8k_text/Flickr_8k.testImages.txt\")\n",
    "\n",
    "file_name = None\n",
    "if feature_model.lower() == 'custom':\n",
    "    file_name = 'cnn_features.pkl' \n",
    "if feature_model.lower() == 'resnet50':\n",
    "    file_name = 'resnet_features.pkl'\n",
    "else:\n",
    "    file_name = 'vgg16_features.pkl'\n",
    "    \n",
    "train_caption_dict = load_captions(train, 1)\n",
    "train_features = load_image_features(train,file_name)\n",
    "dev_caption_dict = load_captions(dev, 1)\n",
    "dev_features = load_image_features(dev, file_name)\n",
    "test_caption_dict = load_captions(test, 0)\n",
    "test_features = load_image_features(test, file_name)\n",
    "\n",
    "\n",
    "\n",
    "#Preparing the Embeddings\n",
    "path_to_glove_file = \"archive-2/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "#Initialize embedding matrix\n",
    "embed_dim = 100\n",
    "vocab_size = 3001\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "tokenizer = create_tokenizer(train_caption_dict, num_vocab=vocab_size)\n",
    "coverage = set()\n",
    "for i in range(1, vocab_size):\n",
    "    w = tokenizer.index_word[i]\n",
    "    if w in embeddings_index.keys():\n",
    "        embedding_matrix[i] = embeddings_index.get(w) #use the pre-trained embeddings, set 0 otherwise\n",
    "        coverage.add(w)\n",
    "\n",
    "max_length = max([len(c.split()) for c in captions_to_list(train_caption_dict)]) - 1 #Maximum length of input sequence\n",
    "\n",
    "#Save the tokenizer for caption_generation\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "\n",
    "#Training:\n",
    "num_epoches = 1\n",
    "batch_size = 32 #photos per batch\n",
    "num_photos = 32\n",
    "steps_per_epoch = int(len(train_caption_dict) / num_photos)\n",
    "val_steps = int(len(dev_caption_dict) / num_photos)\n",
    "\n",
    "\n",
    "if sequence_model.lower() == 'lstm':\n",
    "    model = define_model_lstm(max_length, vocab_size, dp_rate=0.1, embed_size=embed_dim, embedding_matrix=embedding_matrix)\n",
    "    \n",
    "else:\n",
    "    model = define_model_rnn(max_length, vocab_size, dp_rate=0.1, embed_size=embed_dim, embedding_matrix=embedding_matrix)\n",
    "\n",
    "    \n",
    "train_generator = generate_dataset(train_caption_dict, train_features, tokenizer, max_length, vocab_size, batch_size)\n",
    "dev_generator = generate_dataset(dev_caption_dict, dev_features, tokenizer, max_length, vocab_size, batch_size )\n",
    "\n",
    "for i in range(num_epoches):\n",
    "    hist = model.fit(train_generator, steps_per_epoch = steps_per_epoch, epochs=5, verbose=1)\n",
    "\n",
    "    train_loss = hist.history['loss'][-1]\n",
    "    dev_loss = model.evaluate_generator(dev_generator, steps = val_steps, verbose=1)\n",
    "    print(\"The dev_loss at {i}-th epoch: {dev_loss}\".format(i=i, dev_loss=np.round(dev_loss,2)))\n",
    "model.save(f\"{feature_model}_{sequence_model}_model.keras\")\n",
    "        \n",
    "        \n",
    "#Evaluating the model\n",
    "evaluate_model(model, tokenizer, test_caption_dict, test_features, max_length, vocab_size)\n",
    "\n",
    "Fine-tuning on embedding layers\n",
    "\n",
    "model.get_layer('text_embedding').trainable = True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "for i in range(5):\n",
    "    hist = model.fit(train_generator, steps_per_epoch = steps_per_epoch, epochs=5, verbose=1)\n",
    "\n",
    "    train_loss = hist.history['loss'][-1]\n",
    "    dev_loss = model.evaluate(dev_generator, steps = val_steps, verbose=1)\n",
    "    print(\"The dev_loss at {i}-th epoch: {dev_loss}\".format(i=i, dev_loss=np.round(dev_loss,2)))\n",
    "\n",
    "model.save(f\"enhanced_{feature_model}_{sequence_model}_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e7a2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9426cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9834f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
